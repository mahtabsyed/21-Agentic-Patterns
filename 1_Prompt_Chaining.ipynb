{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Prompt Chaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_openai import ChatOpenAI # Or import your preferred model like ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always prioritize clear documentation and code readability to enhance collaboration and maintainability in software projects.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify if API key is present\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    raise ValueError(\"OpenAI API key not found. Please check your .env file.\")\n",
    "\n",
    "# Initialize the OpenAI client and make the API call\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello OpenAI!, Write a 1 line small tip to highlight best software engineering practices!\"}\n",
    "    ]\n",
    ")\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Initialize the language model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the appropriate class and model name for your provider (e.g., ChatGoogleGenerativeAI(model=\"gemini-pro\"))\n",
    "# Setting temperature to control creativity (0.7 is a common balance)\n",
    "try:\n",
    "    # Example for OpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    # Example for Google (uncomment and replace if using Google)\n",
    "    # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
    "    print(f\"Language model initialized: {llm.model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\")\n",
    "    print(\"Please ensure your API key is set correctly and the model name is valid.\")\n",
    "    llm = None # Set llm to None if initialization fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Prompt Templates ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the first prompt template: Rephrase the question\n",
    "# This template takes the original user question as input\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert at rephrasing questions to be clear, concise, and optimized for an AI assistant to answer.\"),\n",
    "    (\"user\", \"Rephrase the following question: {original_question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second prompt template: Answer the rephrased question\n",
    "# This template takes the output from the rephrasing step as input\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Answer the following question based on your knowledge.\"),\n",
    "    (\"user\", \"{rephrased_question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Building the Chain using LangChain Expression Language (LCEL) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at rephrasing questions to be clear, concise, and optimized for an AI assistant to answer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, template='Rephrase the following question: {original_question}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12020c170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1202206e0>, root_client=<openai.OpenAI object at 0x117ebfd90>, root_async_client=<openai.AsyncOpenAI object at 0x117e83770>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: The first part of the chain takes the original question,\n",
    "# applies the rephrase_prompt, sends it to the LLM, and parses the output to a string.\n",
    "rephrase_chain = rephrase_prompt | llm | StrOutputParser()\n",
    "print(rephrase_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  rephrased_question: ChatPromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at rephrasing questions to be clear, concise, and optimized for an AI assistant to answer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_question'], input_types={}, partial_variables={}, template='Rephrase the following question: {original_question}'), additional_kwargs={})])\n",
      "                      | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12020c170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1202206e0>, root_client=<openai.OpenAI object at 0x117ebfd90>, root_async_client=<openai.AsyncOpenAI object at 0x117e83770>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "                      | StrOutputParser()\n",
      "} middle=[ChatPromptTemplate(input_variables=['rephrased_question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Answer the following question based on your knowledge.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['rephrased_question'], input_types={}, partial_variables={}, template='{rephrased_question}'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x12020c170>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1202206e0>, root_client=<openai.OpenAI object at 0x117ebfd90>, root_async_client=<openai.AsyncOpenAI object at 0x117e83770>, model_name='gpt-4o-mini', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Now, chain the output of the rephrase_chain to the answer_prompt\n",
    "# The output of `rephrase_chain` (the rephrased question string) needs to be passed\n",
    "# as the value for the `rephrased_question` variable in the `answer_prompt` template.\n",
    "# LCEL allows us to do this mapping explicitly using a dictionary structure.\n",
    "# The key in the dictionary (\"rephrased_question\") must match the input variable name\n",
    "# expected by the next component (answer_prompt). The value is the preceding chain.\n",
    "full_chain = {\"rephrased_question\": rephrase_chain} | answer_prompt | llm | StrOutputParser()\n",
    "print(full_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Testing the Chain ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question: Tell me about the big statue in Paris people go to see?\n",
      "------------------------------\n",
      "Rephrased question: What is the large statue in Paris that attracts many visitors?\n",
      "------------------------------\n",
      "------------------------------\n",
      "Final answer: The large statue in Paris that attracts many visitors is the **Statue of Liberty** located on Liberty Island in the Seine River. It is a smaller replica of the original Statue of Liberty in New York City, which was a gift from France to the United States. The Paris statue was gifted by the French in 1889 and stands at approximately 11.5 meters (37 feet) tall, not including its pedestal. Visitors often come to see this iconic symbol of freedom and friendship between the two nations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if llm: # Only run if the model was initialized successfully\n",
    "    original_question = \"Tell me about the big statue in Paris people go to see?\"\n",
    "\n",
    "    print(f\"Original question: {original_question}\")\n",
    "    print(\"-\" * 30)\n",
    "       \n",
    "\n",
    "    try:\n",
    "         # First, get the rephrased question\n",
    "        rephrased = rephrase_chain.invoke({\"original_question\": original_question})\n",
    "        print(f\"Rephrased question: {rephrased}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Invoke the chain with the initial input\n",
    "        # Then get the final answer\n",
    "        response = full_chain.invoke({\"original_question\": original_question})\n",
    "        print(f\"Final answer: {response}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during chain execution: {e}\")\n",
    "        print(\"Please check your API key, model name, and network connection.\")\n",
    "else:\n",
    "    print(\"Chain execution skipped due to LLM initialization failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Potential Extensions (Code Concepts - Explained in Text) ---\n",
    "# - Adding Intermediate Logic: Insert Python functions using '|' in LCEL.\n",
    "#   Example: rephrase_chain | some_python_function | {\"rephrased_question\": ...} | ...\n",
    "# - Error Handling: Use .with_fallbacks() or try/except blocks around chain invocation.\n",
    "# - State Management with LangGraph: For non-linear or stateful workflows,\n",
    "#   LangGraph provides a more advanced canvas to define nodes and transitions.\n",
    "#   This involves defining a graph state, nodes (LLM calls, functions, tools),\n",
    "#   and edges (transitions, including conditional ones).\n",
    "#   (Full LangGraph example is more complex and would be shown in a dedicated section or chapter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
